{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "NNoR8XerbUka"
   },
   "source": [
    "# Optimization - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true,
    "id": "TeGFrxrobUkf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "wxqfj_ArbUkh"
   },
   "source": [
    "## Your task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "AcCHQspjbUkh"
   },
   "source": [
    "In this notebook code skeleton for performing logistic regression with gradient descent is given. \n",
    "Your task is to complete the functions where required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "59_LJxZabUki"
   },
   "source": [
    "For numerical reasons, we actually minimize the following loss function\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{w}) = \\frac{1}{N} NLL(\\mathbf{w}) +  \\frac{1}{2}\\lambda ||\\mathbf{w}||^2_2$$\n",
    "\n",
    "where $NLL(\\mathbf{w})$ is the negative log-likelihood function\n",
    "\n",
    "Note: NLL = -log(Likelihood) and Likelihood for logistic regression can be found in the Lecture slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "TIUdQLMqbUkj"
   },
   "source": [
    "## Load and preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "XhyY9BlRbUkj"
   },
   "source": [
    "In this assignment we will work with the UCI ML Breast Cancer Wisconsin (Diagnostic) dataset\n",
    "\n",
    "Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. There are 212 malignant examples and 357 benign examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true,
    "id": "9jGMDh9zbUkk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Add a vector of ones to the data matrix to absorb the bias term\n",
    "X = np.hstack([np.ones([X.shape[0], 1]), X])\n",
    "\n",
    "# Set the random seed so that we have reproducible experiments\n",
    "np.random.seed(123)\n",
    "\n",
    "# Split into train and test\n",
    "test_size = 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "o2mIgdc5bUkl"
   },
   "source": [
    "## Task 1: Implement the sigmoid function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true,
    "id": "M72S6JePbUkm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"\n",
    "    Applies the sigmoid function elementwise to the input data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    t : array, arbitrary shape\n",
    "        Input data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    t_sigmoid : array, arbitrary shape.\n",
    "        Data after applying the sigmoid function.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    sigmoid = []\n",
    "    for i in range(len(t)):\n",
    "        prob = 1/(1+np.exp(-t))\n",
    "        sigmoid.append(prob)\n",
    "    return sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "9BxtNy0ZbUkn"
   },
   "source": [
    "## Task 2: Implement the negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true,
    "id": "50H6Q7brbUkn",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def negative_log_likelihood(X, y, w):\n",
    "    \"\"\"\n",
    "    Negative Log Likelihood of the Logistic Regression.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape [N, D]\n",
    "        (Augmented) feature matrix.\n",
    "    y : array, shape [N]\n",
    "        Classification targets.\n",
    "    w : array, shape [D]\n",
    "        Regression coefficients (w[0] is the bias term).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    nll : float\n",
    "        The negative log likelihood.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    negativeLog = 0\n",
    "    probabilities = sigmoid(np.dot(X,w))\n",
    "    for i in range(len(probabilities)):\n",
    "        negativeLog = y[i]*np.log(probabilities[i])+ (1-y[i])*np.log(1-probabilities[i])\n",
    "    negativeLog *= -1\n",
    "    return(negativeLog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "1oFN9u9SbUkn"
   },
   "source": [
    "### Computing the loss function $\\mathcal{L}(\\mathbf{w})$ (nothing to do here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true,
    "id": "mAL-9JozbUko",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_loss(X, y, w, lmbda):\n",
    "    \"\"\"\n",
    "    Negative Log Likelihood of the Logistic Regression.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape [N, D]\n",
    "        (Augmented) feature matrix.\n",
    "    y : array, shape [N]\n",
    "        Classification targets.\n",
    "    w : array, shape [D]\n",
    "        Regression coefficients (w[0] is the bias term).\n",
    "    lmbda : float\n",
    "        L2 regularization strength.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "        Loss of the regularized logistic regression model.\n",
    "    \"\"\"\n",
    "    # The bias term w[0] is not regularized by convention\n",
    "    return negative_log_likelihood(X, y, w) / len(y) + lmbda * 0.5 * np.linalg.norm(w[1:])**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "Pp8MaXNebUkp"
   },
   "source": [
    "## Task 3: Implement the gradient $\\nabla_{\\mathbf{w}}\\mathcal{L}(\\mathbf{w})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "Hcci90YhbUkp"
   },
   "source": [
    "Make sure that you compute the gradient of the loss function $\\mathcal{L}(\\mathbf{w})$ (not simply the NLL!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true,
    "id": "Lp0YjH-qbUkp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_gradient(X, y, w, mini_batch_indices, lmbda):\n",
    "    \"\"\"\n",
    "    Calculates the gradient (full or mini-batch) of the negative log likelilhood w.r.t. w.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape [N, D]\n",
    "        (Augmented) feature matrix.\n",
    "    y : array, shape [N]\n",
    "        Classification targets.\n",
    "    w : array, shape [D]\n",
    "        Regression coefficients (w[0] is the bias term).\n",
    "    mini_batch_indices: array, shape [mini_batch_size]\n",
    "        The indices of the data points to be included in the (stochastic) calculation of the gradient.\n",
    "        This includes the full batch gradient as well, if mini_batch_indices = np.arange(n_train).\n",
    "    lmbda: float\n",
    "        Regularization strentgh. lmbda = 0 means having no regularization.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dw : array, shape [D]\n",
    "        Gradient w.r.t. w.\n",
    "    \"\"\"\n",
    "    gradient = np.zeros(w.shape)\n",
    "    m = len(mini_batch_indices)\n",
    "    for i in mini_batch_indices:\n",
    "        gradient[0]+= (sigmoid(np.dot(X,w))[0][i]-y[i])*X[i][0]/m\n",
    "        \n",
    "    for j in range(1,len(X[0])): # starting from 1 since i calculated for gradient[0] above\n",
    "        for i in mini_batch_indices:\n",
    "            gradient[j]+= (sigmoid(np.dot(X,w))[0][i]-y[i])*X[i][j]/m +lmbda*w[j]\n",
    "        \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "1tBqOwkObUkq"
   },
   "source": [
    "### Train the logistic regression model (nothing to do here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true,
    "id": "NHxQATD1bUkq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def logistic_regression(X, y, num_steps, learning_rate, mini_batch_size, lmbda, verbose):\n",
    "    \"\"\"\n",
    "    Performs logistic regression with (stochastic) gradient descent.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape [N, D]\n",
    "        (Augmented) feature matrix.\n",
    "    y : array, shape [N]\n",
    "        Classification targets.\n",
    "    num_steps : int\n",
    "        Number of steps of gradient descent to perform.\n",
    "    learning_rate: float\n",
    "        The learning rate to use when updating the parameters w.\n",
    "    mini_batch_size: int\n",
    "        The number of examples in each mini-batch.\n",
    "        If mini_batch_size=n_train we perform full batch gradient descent. \n",
    "    lmbda: float\n",
    "        Regularization strentgh. lmbda = 0 means having no regularization.\n",
    "    verbose : bool\n",
    "        Whether to print the loss during optimization.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    w : array, shape [D]\n",
    "        Optimal regression coefficients (w[0] is the bias term).\n",
    "    trace: list\n",
    "        Trace of the loss function after each step of gradient descent.\n",
    "    \"\"\"\n",
    "    \n",
    "    trace = [] # saves the value of loss every 50 iterations to be able to plot it later\n",
    "    n_train = X.shape[0] # number of training instances\n",
    "    \n",
    "    w = np.zeros(X.shape[1]) # initialize the parameters to zeros\n",
    "    \n",
    "    # run gradient descent for a given number of steps\n",
    "    for step in range(num_steps):\n",
    "        permuted_idx = np.random.permutation(n_train) # shuffle the data\n",
    "        \n",
    "        # go over each mini-batch and update the paramters\n",
    "        # if mini_batch_size = n_train we perform full batch GD and this loop runs only once\n",
    "        for idx in range(0, n_train, mini_batch_size):\n",
    "            # get the random indices to be included in the mini batch\n",
    "            mini_batch_indices = permuted_idx[idx:idx+mini_batch_size]\n",
    "            gradient = get_gradient(X, y, w, mini_batch_indices, lmbda)\n",
    "\n",
    "            # update the parameters\n",
    "            w = w - learning_rate * gradient\n",
    "        \n",
    "        # calculate and save the current loss value every 50 iterations\n",
    "        if step % 50 == 0:\n",
    "            loss = compute_loss(X, y, w, lmbda)\n",
    "            trace.append(loss)\n",
    "            # print loss to monitor the progress\n",
    "            if verbose:\n",
    "                print('Step {0}, loss = {1:.4f}'.format(step, loss))\n",
    "    return w, trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "aYcy9tBNbUkr"
   },
   "source": [
    "## Task 4: Implement the function to obtain the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true,
    "id": "vIQdh7Z_bUkr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(X, w):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape [N_test, D]\n",
    "        (Augmented) feature matrix.\n",
    "    w : array, shape [D]\n",
    "        Regression coefficients (w[0] is the bias term).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    y_pred : array, shape [N_test]\n",
    "        A binary array of predictions.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    y_pred = np.zeros(y.shape)\n",
    "    for i in range(len(X)):\n",
    "        if sigmoid(X)>0.5:\n",
    "            y[i] = 1\n",
    "        else:\n",
    "            y[i] = 0\n",
    "    return(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "dtW83QThbUks"
   },
   "source": [
    "### Full batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true,
    "id": "1LyOpSyubUks",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Change this to True if you want to see loss values over iterations.\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true,
    "id": "CWy9IVZvbUks",
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-74492d924b26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mn_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m w_full, trace_full = logistic_regression(X_train, \n\u001b[0m\u001b[0;32m      3\u001b[0m                                          \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                          \u001b[0mnum_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                          \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-d9b8dd3faa2b>\u001b[0m in \u001b[0;36mlogistic_regression\u001b[1;34m(X, y, num_steps, learning_rate, mini_batch_size, lmbda, verbose)\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;31m# get the random indices to be included in the mini batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mmini_batch_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpermuted_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmini_batch_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlmbda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[1;31m# update the parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-88963b9ce5e6>\u001b[0m in \u001b[0;36mget_gradient\u001b[1;34m(X, y, w, mini_batch_indices, lmbda)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# starting from 1 since i calculated for gradient[0] above\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmini_batch_indices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0mgradient\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0mlmbda\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_train = X_train.shape[0]\n",
    "w_full, trace_full = logistic_regression(X_train, \n",
    "                                         y_train, \n",
    "                                         num_steps=8000, \n",
    "                                         learning_rate=1e-5, \n",
    "                                         mini_batch_size=n_train, \n",
    "                                         lmbda=0.1,\n",
    "                                         verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "2UCJHuhObUkt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_train = X_train.shape[0]\n",
    "w_minibatch, trace_minibatch = logistic_regression(X_train, \n",
    "                                                   y_train, \n",
    "                                                   num_steps=8000, \n",
    "                                                   learning_rate=1e-5, \n",
    "                                                   mini_batch_size=50, \n",
    "                                                   lmbda=0.1,\n",
    "                                                   verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "WgHLS2JFbUkt"
   },
   "source": [
    "Our reference solution produces, but don't worry if yours is not exactly the same. \n",
    "\n",
    "    Full batch: accuracy: 0.9240, f1_score: 0.9384\n",
    "    Mini-batch: accuracy: 0.9415, f1_score: 0.9533"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "3K1f0bLebUkt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_full = predict(X_test, w_full)\n",
    "y_pred_minibatch = predict(X_test, w_minibatch)\n",
    "\n",
    "print('Full batch: accuracy: {:.4f}, f1_score: {:.4f}'\n",
    "      .format(accuracy_score(y_test, y_pred_full), f1_score(y_test, y_pred_full)))\n",
    "print('Mini-batch: accuracy: {:.4f}, f1_score: {:.4f}'\n",
    "      .format(accuracy_score(y_test, y_pred_minibatch), f1_score(y_test, y_pred_minibatch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "_R9AO6b3bUku",
    "outputId": "1ffbbcea-7b96-4cd9-c10f-3bf948e10ec1"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 10])\n",
    "plt.plot(trace_full, label='Full batch')\n",
    "plt.plot(trace_minibatch, label='Mini-batch')\n",
    "plt.xlabel('Iterations * 50')\n",
    "plt.ylabel('Loss $\\mathcal{L}(\\mathbf{w})$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "logistic_regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
